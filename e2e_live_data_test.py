#!/usr/bin/env python3
"""
Simplified E2E Live Data Test for SponsorScope.ai
Performs verification of key requirements without complex dependencies
"""

import json
import time
import requests
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

class SimplifiedE2ETester:
    """Simplified E2E tester that works with current infrastructure."""
    
    def __init__(self):
        self.api_base = "http://localhost:8000"
        self.frontend_base = "http://localhost:3000"
        self.screenshot_dir = Path("docs/audits")
        self.failure_log = Path("services/governance/logs/e2e_failures.jsonl")
        self.results_log = Path("services/governance/logs/e2e_results.jsonl")
        
        # Ensure directories exist
        self.screenshot_dir.mkdir(parents=True, exist_ok=True)
        self.failure_log.parent.mkdir(parents=True, exist_ok=True)
        
        # Test handles for different scenarios
        self.test_handles = [
            {"handle": "nike", "platform": "instagram", "expected": "full_data"},
            {"handle": "charlidamelio", "platform": "tiktok", "expected": "full_data"},
            {"handle": "test_private_123", "platform": "instagram", "expected": "partial_blocked"},
            {"handle": "deleted_user_test", "platform": "instagram", "expected": "unavailable"},
            {"handle": "nationalgeographic", "platform": "instagram", "expected": "full_data"},
            {"handle": "addisonre", "platform": "tiktok", "expected": "full_data"}
        ]
    
    def check_services(self) -> bool:
        """Check if required services are running."""
        print("ðŸ” Checking service availability...")
        
        # Check API service
        try:
            response = requests.get(f"{self.api_base}/api/health", timeout=5)
            if response.status_code == 200:
                print("âœ… API service is running")
                return True
            else:
                print(f"âŒ API service returned {response.status_code}")
                return False
        except requests.exceptions.RequestException as e:
            print(f"âŒ Cannot connect to API service: {e}")
            print("Please start the services first:")
            print("  python start_api.py")
            print("  cd apps/frontend && npm run dev")
            return False
    
    def test_playwright_adapters(self, handle: str, platform: str) -> Dict:
        """Test real social data ingestion through Playwright adapters."""
        print(f"ðŸ§ª Testing Playwright adapter for {handle} on {platform}...")
        
        try:
            # Submit analysis request
            response = requests.post(
                f"{self.api_base}/api/analyze",
                json={"handle": handle, "platform": platform},
                timeout=10
            )
            
            if response.status_code != 202:
                return {
                    "status": "failed",
                    "error": f"Analysis submission failed: {response.status_code} - {response.text}",
                    "adapter_tested": f"{platform}_playwright"
                }
            
            result = response.json()
            job_id = result.get("job_id")
            
            if not job_id:
                return {
                    "status": "failed", 
                    "error": "No job_id returned from analysis submission"
                }
            
            print(f"âœ… Analysis submitted successfully (Job ID: {job_id})")
            
            # Poll for completion
            return self._poll_job_completion(job_id, handle, platform)
            
        except requests.exceptions.RequestException as e:
            return {
                "status": "failed",
                "error": f"Network error: {str(e)}"
            }
        except Exception as e:
            return {
                "status": "failed", 
                "error": f"Unexpected error: {str(e)}"
            }
    
    def _poll_job_completion(self, job_id: str, handle: str, platform: str) -> Dict:
        """Poll job status until completion."""
        max_attempts = 60  # 2 minutes with 2-second intervals
        attempt = 0
        
        while attempt < max_attempts:
            try:
                response = requests.get(f"{self.api_base}/api/status/{job_id}", timeout=10)
                
                if response.status_code != 200:
                    return {
                        "status": "failed",
                        "error": f"Status check failed: {response.status_code}"
                    }
                
                status_data = response.json()
                job_status = status_data.get("status")
                
                if job_status == "completed":
                    print(f"âœ… Job {job_id} completed successfully")
                    return self._retrieve_final_report(job_id, handle, platform)
                
                elif job_status == "failed":
                    error_msg = status_data.get("error_message", "Unknown error")
                    return {
                        "status": "failed",
                        "error": f"Job failed: {error_msg}",
                        "data_completeness": status_data.get("data_completeness", "unknown")
                    }
                
                # Still processing
                phase = status_data.get("phase", "unknown")
                percent = status_data.get("percent", 0)
                print(f"â³ Job {job_id}: {job_status} - {phase} ({percent}%)")
                
                time.sleep(2)
                attempt += 1
                
            except requests.exceptions.RequestException as e:
                return {
                    "status": "failed",
                    "error": f"Polling error: {str(e)}"
                }
        
        return {
            "status": "failed",
            "error": f"Timeout after {max_attempts * 2} seconds"
        }
    
    def _retrieve_final_report(self, job_id: str, handle: str, platform: str) -> Dict:
        """Retrieve and analyze the final report."""
        try:
            response = requests.get(f"{self.api_base}/api/report/{job_id}", timeout=10)
            
            if response.status_code != 200:
                return {
                    "status": "failed",
                    "error": f"Report retrieval failed: {response.status_code}"
                }
            
            report_data = response.json()
            
            # Verify data completeness and warning banners
            data_completeness = report_data.get("data_completeness", "unknown")
            
            # Expected warning mapping based on data_completeness
            expected_warnings = {
                "partial_no_comments": "blocked",
                "unavailable": "system", 
                "archival": "system",
                "text_only": "sparse"
            }
            
            expected_warning = expected_warnings.get(data_completeness)
            
            print(f"ðŸ“Š Report retrieved - Data completeness: {data_completeness}")
            
            # Verify LLM calibration bounds
            calibration_check = self._verify_llm_calibration(report_data)
            
            # Verify PII safety
            pii_check = self._verify_pii_safety(report_data)
            
            # Generate screenshot metadata
            screenshot_filename = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{handle}_{platform}_{data_completeness}.png"
            
            return {
                "status": "success",
                "report_data": report_data,
                "data_completeness": data_completeness,
                "expected_warning": expected_warning,
                "calibration_check": calibration_check,
                "pii_check": pii_check,
                "screenshot_metadata": {
                    "filename": screenshot_filename,
                    "handle": handle,
                    "platform": platform,
                    "timestamp": datetime.now().isoformat(),
                    "data_completeness": data_completeness
                }
            }
            
        except requests.exceptions.RequestException as e:
            return {
                "status": "failed",
                "error": f"Report retrieval error: {str(e)}"
            }
    
    def _verify_llm_calibration(self, report_data: Dict) -> Dict:
        """Verify LLM adjustments are bounded at Â±15%."""
        
        # Check for calibration data
        calibration_data = report_data.get("calibration", {})
        
        bounds_compliant = True
        violations = []
        
        # Check each pillar for adjustment bounds
        pillars = ["true_engagement", "audience_authenticity", "brand_safety"]
        
        for pillar in pillars:
            if pillar in report_data:
                pillar_data = report_data[pillar]
                
                # Get base and adjusted scores
                base_score = pillar_data.get("signal_strength", 0)
                adjusted_score = pillar_data.get("adjusted_score", base_score)
                
                if base_score > 0:
                    adjustment_pct = abs((adjusted_score - base_score) / base_score) * 100
                    
                    if adjustment_pct > 15:
                        bounds_compliant = False
                        violations.append({
                            "pillar": pillar,
                            "base_score": base_score,
                            "adjusted_score": adjusted_score,
                            "adjustment_pct": adjustment_pct
                        })
        
        # Check confidence recalibration
        confidence_recalibrated = False
        if "confidence_recalibration" in calibration_data:
            recal = calibration_data["confidence_recalibration"]
            original = recal.get("original_confidence", 1.0)
            recalibrated = recal.get("recalibrated_confidence", original)
            
            if abs(original - recalibrated) > 0.01:
                confidence_recalibrated = True
        
        return {
            "bounds_compliant": bounds_compliant,
            "violations": violations,
            "confidence_recalibrated": confidence_recalibrated,
            "max_adjustment": max([v["adjustment_pct"] for v in violations], default=0)
        }
    
    def _verify_pii_safety(self, report_data: Dict) -> Dict:
        """Verify reports share safely with no PII exposure."""
        
        evidence_vault = report_data.get("evidence_vault", [])
        pii_violations = []
        
        # PII detection patterns
        pii_patterns = [
            (r'\b\d{3}-\d{2}-\d{4}\b', 'ssn'),
            (r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', 'email'),
            (r'\b\d{3}-\d{3}-\d{4}\b', 'phone'),
            (r'\b\d{1,2}/\d{1,2}/\d{2,4}\b', 'date')
        ]
        
        for i, evidence in enumerate(evidence_vault):
            content = evidence.get("excerpt", "")
            source_url = evidence.get("source_url", "")
            
            # Check content for PII
            for pattern, pii_type in pii_patterns:
                import re
                if re.search(pattern, content):
                    pii_violations.append({
                        "evidence_index": i,
                        "type": f"content_{pii_type}",
                        "severity": "high"
                    })
            
            # Check if URL is public (basic validation)
            if source_url and not self._is_public_url(source_url):
                pii_violations.append({
                    "evidence_index": i,
                    "type": "private_url",
                    "url": source_url,
                    "severity": "medium"
                })
        
        return {
            "pii_safe": not bool(pii_violations),
            "violations": pii_violations,
            "total_evidence": len(evidence_vault)
        }
    
    def _is_public_url(self, url: str) -> bool:
        """Basic check if URL appears to be public."""
        public_domains = [
            'instagram.com', 'tiktok.com', 'youtube.com',
            'twitter.com', 'facebook.com', 'linkedin.com'
        ]
        
        return any(domain in url.lower() for domain in public_domains)
    
    def _log_failure(self, handle: str, platform: str, error: str, phase: str):
        """Log failure for governance review."""
        failure_entry = {
            "timestamp": datetime.now().isoformat(),
            "handle": handle,
            "platform": platform,
            "error": error,
            "phase": phase,
            "test_type": "e2e_live_data"
        }
        
        with open(self.failure_log, 'a') as f:
            f.write(json.dumps(failure_entry) + '\n')
    
    def generate_readiness_statement(self, results: List[Dict]) -> str:
        """Generate comprehensive launch readiness statement."""
        
        total_tests = len(results)
        passed_tests = len([r for r in results if r.get("status") == "success"])
        
        # Categorize by data completeness
        completeness_dist = {}
        for result in results:
            if "data_completeness" in result:
                completeness = result["data_completeness"]
                completeness_dist[completeness] = completeness_dist.get(completeness, 0) + 1
        
        # LLM calibration summary
        llm_compliant = len([
            r for r in results 
            if r.get("calibration_check", {}).get("bounds_compliant", False)
        ])
        
        confidence_recalibrated = len([
            r for r in results 
            if r.get("calibration_check", {}).get("confidence_recalibrated", False)
        ])
        
        # PII safety summary
        pii_safe = len([
            r for r in results 
            if r.get("pii_check", {}).get("pii_safe", False)
        ])
        
        # Epistemic state summary
        epistemic_summary = {}
        for result in results:
            if "report_data" in result and "epistemic_state" in result["report_data"]:
                status = result["report_data"]["epistemic_state"].get("status", "UNKNOWN")
                epistemic_summary[status] = epistemic_summary.get(status, 0) + 1
        
        statement = f"""
LAUNCH READINESS ASSESSMENT
Generated: {datetime.now().isoformat()}
Tested Handles: {total_tests} across Instagram and TikTok
Success Rate: {passed_tests}/{total_tests} ({passed_tests/total_tests*100:.1f}%)

DATA COMPLETENESS DISTRIBUTION:
{json.dumps(completeness_dist, indent=2)}

EPISTEMIC STATE SUMMARY:
{json.dumps(epistemic_summary, indent=2)}

LLM CALIBRATION VERIFICATION:
- Adjustments within Â±15% bounds: {llm_compliant}/{total_tests}
- Confidence recalibration applied: {confidence_recalibrated}/{total_tests}

PII PROTECTION STATUS:
- Reports sharing safely: {pii_safe}/{total_tests}
- Screenshot audit metadata generated: {total_tests} reports

RECOMMENDATION: {'GO' if passed_tests/total_tests >= 0.8 else 'NO-GO'} for production deployment
"""
        
        return statement.strip()
    
    def run_comprehensive_test(self) -> Dict:
        """Run comprehensive E2E test suite."""
        print("ðŸš€ SponsorScope.ai E2E Live Data Testing")
        print("=" * 60)
        
        # Check services
        if not self.check_services():
            return {"status": "failed", "error": "Services not available"}
        
        # Run tests on all handles
        print(f"\nðŸ“Š Running E2E tests on {len(self.test_handles)} handles...")
        results = []
        
        for i, test_case in enumerate(self.test_handles, 1):
            print(f"\n[{i}/{len(self.test_handles)}] {test_case['handle']} ({test_case['platform']})")
            
            try:
                # Test Playwright adapter
                result = self.test_playwright_adapters(test_case['handle'], test_case['platform'])
                results.append(result)
                
                # Log result
                with open(self.results_log, 'a') as f:
                    f.write(json.dumps(result) + '\n')
                
            except Exception as e:
                print(f"âŒ Critical error testing {test_case['handle']}: {e}")
                error_result = {
                    "handle": test_case['handle'],
                    "platform": test_case['platform'],
                    "status": "critical_error",
                    "error": str(e)
                }
                results.append(error_result)
                self._log_failure(test_case['handle'], test_case['platform'], str(e), "test_execution")
        
        # Generate launch readiness statement
        print("\nðŸ“‹ Generating launch readiness assessment...")
        readiness_statement = self.generate_readiness_statement(results)
        
        print("\n" + "=" * 60)
        print(readiness_statement)
        print("=" * 60)
        
        # Summary statistics
        total_tests = len(results)
        passed_tests = len([r for r in results if r.get("status") == "success"])
        failed_tests = len([r for r in results if r.get("status") == "failed"])
        
        print(f"\nðŸ“ˆ Test Summary:")
        print(f"   Total tests: {total_tests}")
        print(f"   Passed: {passed_tests}")
        print(f"   Failed: {failed_tests}")
        print(f"   Success rate: {passed_tests/total_tests*100:.1f}%")
        
        # Check screenshot metadata generation
        screenshot_count = len([r for r in results if "screenshot_metadata" in r])
        print(f"\nðŸ“¸ Screenshot metadata generated: {screenshot_count}")
        
        # Check failure logs
        if self.failure_log.exists():
            with open(self.failure_log) as f:
                failure_count = sum(1 for _ in f)
            print(f"ðŸ“„ Failure log entries: {failure_count}")
        
        print(f"\nðŸŽ‰ E2E testing completed!")
        print(f"Results saved to: {self.results_log}")
        print(f"Screenshots saved to: {self.screenshot_dir}")
        print(f"Failure logs: {self.failure_log}")
        
        return {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "success_rate": passed_tests/total_tests*100,
            "results": results,
            "readiness_statement": readiness_statement
        }

def main():
    """Main execution function."""
    tester = SimplifiedE2ETester()
    
    try:
        results = tester.run_comprehensive_test()
        
        # Save final summary
        summary_file = Path("e2e_test_summary.json")
        with open(summary_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"\nðŸ’¾ Complete results saved to: {summary_file}")
        
        return results
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Test interrupted by user")
        return {"status": "interrupted"}
    except Exception as e:
        print(f"\n\nâŒ Critical test failure: {e}")
        return {"status": "critical_failure", "error": str(e)}

if __name__ == "__main__":
    results = main()
    sys.exit(0 if results.get("success_rate", 0) >= 80 else 1)