#!/usr/bin/env python3
"""
Test Data Completeness Signaling for SponsorScope.ai
Tests profiles with comments disabled and recently created accounts
"""

import json
import time
import requests
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

class DataCompletenessTester:
    """Test data completeness signaling with specific profile conditions."""
    
    def __init__(self):
        self.api_base = "http://localhost:8000"
        self.frontend_base = "http://localhost:3000"
        self.screenshot_dir = Path("docs/audits")
        self.failure_log = Path("services/governance/logs/e2e_failures.jsonl")
        self.results_log = Path("services/governance/logs/e2e_results.jsonl")
        
        # Ensure directories exist
        self.screenshot_dir.mkdir(parents=True, exist_ok=True)
        self.failure_log.parent.mkdir(parents=True, exist_ok=True)
        
        # Test profiles for data completeness signaling
        self.test_profiles = [
            {
                "handle": "test_no_comments_001",
                "platform": "instagram", 
                "scenario": "comments_disabled",
                "expected_completeness": "partial_no_comments",
                "expected_warning": "blocked",
                "description": "Profile with comments disabled"
            },
            {
                "handle": "new_user_2024_001", 
                "platform": "instagram",
                "scenario": "recently_created",
                "expected_completeness": "sparse",
                "expected_warning": "sparse",
                "description": "Recently created account with minimal data"
            },
            {
                "handle": "test_private_001",
                "platform": "instagram",
                "scenario": "private_account", 
                "expected_completeness": "unavailable",
                "expected_warning": "private",
                "description": "Private account with access denied"
            },
            {
                "handle": "test_archival_001",
                "platform": "instagram",
                "scenario": "archival_data",
                "expected_completeness": "archival", 
                "expected_warning": "system",
                "description": "Account with archival data only"
            }
        ]
    
    def check_services(self) -> bool:
        """Check if required services are running."""
        print("ðŸ” Checking service availability...")
        
        # Check API service
        try:
            response = requests.get(f"{self.api_base}/api/health", timeout=5)
            if response.status_code == 200:
                print("âœ… API service is running")
                return True
            else:
                print(f"âŒ API service returned {response.status_code}")
                return False
        except requests.exceptions.RequestException as e:
            print(f"âŒ Cannot connect to API service: {e}")
            print("Please start the services first:")
            print("  python start_api.py")
            print("  cd apps/frontend && npm run dev")
            return False
    
    def test_data_completeness_signaling(self, profile: Dict) -> Dict:
        """Test data completeness signaling for a specific profile scenario."""
        print(f"ðŸ§ª Testing {profile['description']}...")
        
        try:
            # Submit analysis request
            response = requests.post(
                f"{self.api_base}/api/analyze",
                json={"handle": profile['handle'], "platform": profile['platform']},
                timeout=10
            )
            
            if response.status_code != 202:
                return {
                    "status": "failed",
                    "error": f"Analysis submission failed: {response.status_code} - {response.text}",
                    "profile": profile
                }
            
            result = response.json()
            job_id = result.get("job_id")
            
            if not job_id:
                return {
                    "status": "failed", 
                    "error": "No job_id returned from analysis submission",
                    "profile": profile
                }
            
            print(f"âœ… Analysis submitted (Job ID: {job_id})")
            
            # Poll for completion
            return self._poll_job_with_completeness_check(job_id, profile)
            
        except requests.exceptions.RequestException as e:
            return {
                "status": "failed",
                "error": f"Network error: {str(e)}",
                "profile": profile
            }
        except Exception as e:
            return {
                "status": "failed", 
                "error": f"Unexpected error: {str(e)}",
                "profile": profile
            }
    
    def _poll_job_with_completeness_check(self, job_id: str, profile: Dict) -> Dict:
        """Poll job status and verify data completeness signaling."""
        max_attempts = 60  # 2 minutes with 2-second intervals
        attempt = 0
        
        while attempt < max_attempts:
            try:
                response = requests.get(f"{self.api_base}/api/status/{job_id}", timeout=10)
                
                if response.status_code != 200:
                    return {
                        "status": "failed",
                        "error": f"Status check failed: {response.status_code}"
                    }
                
                status_data = response.json()
                job_status = status_data.get("status")
                
                if job_status == "completed":
                    print(f"âœ… Job {job_id} completed")
                    return self._verify_data_completeness_signaling(job_id, profile)
                
                elif job_status == "failed":
                    error_msg = status_data.get("error_message", "Unknown error")
                    return {
                        "status": "failed",
                        "error": f"Job failed: {error_msg}",
                        "data_completeness": status_data.get("data_completeness", "unknown"),
                        "profile": profile
                    }
                
                # Still processing
                phase = status_data.get("phase", "unknown")
                percent = status_data.get("percent", 0)
                print(f"â³ {phase} ({percent}%)")
                
                time.sleep(2)
                attempt += 1
                
            except requests.exceptions.RequestException as e:
                return {
                    "status": "failed",
                    "error": f"Polling error: {str(e)}"
                }
        
        return {
            "status": "failed",
            "error": f"Timeout after {max_attempts * 2} seconds"
        }
    
    def _verify_data_completeness_signaling(self, job_id: str, profile: Dict) -> Dict:
        """Verify data completeness signaling is working correctly."""
        try:
            response = requests.get(f"{self.api_base}/api/report/{job_id}", timeout=10)
            
            if response.status_code != 200:
                return {
                    "status": "failed",
                    "error": f"Report retrieval failed: {response.status_code}"
                }
            
            report_data = response.json()
            
            # Extract key metrics for verification
            data_completeness = report_data.get("data_completeness", "unknown")
            confidence = report_data.get("true_engagement", {}).get("confidence", 1.0)
            warning_type = self._determine_expected_warning(data_completeness)
            
            # Verify partial data detection
            partial_data_detected = data_completeness in ["partial_no_comments", "sparse", "text_only"]
            
            # Verify warning is displayed
            warning_expected = warning_type is not None
            
            # Verify confidence is reduced for partial data
            confidence_reduced = confidence < 0.8 if partial_data_detected else confidence >= 0.8
            
            # Verify no score appears without uncertainty context for partial data
            score_without_uncertainty = self._check_score_without_uncertainty(report_data, partial_data_detected)
            
            # Generate screenshot filename
            screenshot_filename = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{profile['handle']}_{data_completeness}.png"
            
            return {
                "status": "success",
                "profile": profile,
                "report_data": report_data,
                "verification_results": {
                    "data_completeness": data_completeness,
                    "expected_completeness": profile["expected_completeness"],
                    "completeness_match": data_completeness == profile["expected_completeness"],
                    "partial_data_detected": partial_data_detected,
                    "warning_displayed": warning_expected,
                    "expected_warning_type": profile["expected_warning"],
                    "actual_warning_type": warning_type,
                    "confidence_reduced": confidence_reduced,
                    "confidence_score": confidence,
                    "score_without_uncertainty": score_without_uncertainty,
                    "ux_compliance": self._assess_ux_compliance(report_data, profile)
                },
                "screenshot_metadata": {
                    "filename": screenshot_filename,
                    "handle": profile["handle"],
                    "platform": profile["platform"],
                    "scenario": profile["scenario"],
                    "data_completeness": data_completeness,
                    "timestamp": datetime.now().isoformat()
                }
            }
            
        except requests.exceptions.RequestException as e:
            return {
                "status": "failed",
                "error": f"Report retrieval error: {str(e)}"
            }
    
    def _determine_expected_warning(self, data_completeness: str) -> Optional[str]:
        """Determine expected warning type based on data completeness."""
        warning_mapping = {
            "partial_no_comments": "blocked",
            "sparse": "sparse", 
            "text_only": "sparse",
            "archival": "system",
            "unavailable": "private"
        }
        return warning_mapping.get(data_completeness)
    
    def _check_score_without_uncertainty(self, report_data: Dict, partial_data_detected: bool) -> bool:
        """Check if scores appear without uncertainty context for partial data."""
        if not partial_data_detected:
            return False
        
        # Check if uncertainty bands are present for partial data
        pillars = ["true_engagement", "audience_authenticity", "brand_safety"]
        
        for pillar in pillars:
            if pillar in report_data:
                pillar_data = report_data[pillar]
                # Check if confidence is present and uncertainty is properly indicated
                if "confidence" in pillar_data and pillar_data["confidence"] < 0.8:
                    # For partial data, should have uncertainty indication
                    if not ("uncertainty_band" in pillar_data or "confidence" in pillar_data):
                        return True
        
        return False
    
    def _assess_ux_compliance(self, report_data: Dict, profile: Dict) -> Dict:
        """Assess UX compliance for data completeness signaling."""
        data_completeness = report_data.get("data_completeness", "unknown")
        confidence = report_data.get("true_engagement", {}).get("confidence", 1.0)
        
        compliance_checks = {
            "warning_displayed": data_completeness in ["partial_no_comments", "sparse", "unavailable", "archival"],
            "confidence_appropriate": confidence < 0.8 if data_completeness in ["partial_no_comments", "sparse"] else confidence >= 0.8,
            "uncertainty_indicated": data_completeness in ["partial_no_comments", "sparse", "text_only"],
            "no_misleading_scores": True,  # Will be checked in verification
            "proper_explanation": data_completeness != "unknown"
        }
        
        overall_compliance = all(compliance_checks.values())
        
        return {
            "overall_compliance": overall_compliance,
            "checks": compliance_checks,
            "data_completeness": data_completeness,
            "confidence": confidence
        }
    
    def generate_compliance_report(self, results: List[Dict]) -> str:
        """Generate comprehensive data completeness compliance report."""
        
        total_tests = len(results)
        passed_tests = len([r for r in results if r.get("status") == "success"])
        
        # Analyze verification results
        verification_summary = []
        
        for result in results:
            if result.get("status") == "success" and "verification_results" in result:
                verif = result["verification_results"]
                verification_summary.append({
                    "scenario": result["profile"]["scenario"],
                    "data_completeness": verif["data_completeness"],
                    "partial_data_detected": verif["partial_data_detected"],
                    "warning_displayed": verif["warning_displayed"],
                    "confidence_reduced": verif["confidence_reduced"],
                    "ux_compliance": verif["ux_compliance"]["overall_compliance"]
                })
        
        # Generate report
        report = f"""
DATA COMPLETENESS SIGNALING TEST REPORT
Generated: {datetime.now().isoformat()}

TEST SUMMARY:
- Total profiles tested: {total_tests}
- Successful tests: {passed_tests}
- Success rate: {passed_tests/total_tests*100:.1f}%

VERIFICATION RESULTS:
{json.dumps(verification_summary, indent=2)}

KEY FINDINGS:
- Partial data detection: {'âœ… WORKING' if all(s['partial_data_detected'] for s in verification_summary if s['scenario'] in ['comments_disabled', 'recently_created']) else 'âŒ FAILED'}
- Warning display: {'âœ… WORKING' if all(s['warning_displayed'] for s in verification_summary) else 'âŒ FAILED'}
- Confidence reduction: {'âœ… WORKING' if all(s['confidence_reduced'] for s in verification_summary if s['scenario'] in ['comments_disabled', 'recently_created']) else 'âŒ FAILED'}
- UX compliance: {'âœ… COMPLIANT' if all(s['ux_compliance'] for s in verification_summary) else 'âŒ NON-COMPLIANT'}

SCREENSHOTS GENERATED: {len([r for r in results if 'screenshot_metadata' in r])}
"""
        
        return report.strip()
    
    def run_data_completeness_tests(self) -> Dict:
        """Run comprehensive data completeness signaling tests."""
        print("ðŸ” DATA COMPLETENESS SIGNALING TESTS")
        print("=" * 60)
        
        # Check services
        if not self.check_services():
            return {"status": "failed", "error": "Services not available"}
        
        # Run tests on all profiles
        print(f"\nðŸ“Š Testing {len(self.test_profiles)} data completeness scenarios...")
        results = []
        
        for i, profile in enumerate(self.test_profiles, 1):
            print(f"\n[{i}/{len(self.test_profiles)}] {profile['description']}")
            print(f"   Handle: {profile['handle']} | Platform: {profile['platform']}")
            print(f"   Expected: {profile['expected_completeness']} | Warning: {profile['expected_warning']}")
            
            try:
                result = self.test_data_completeness_signaling(profile)
                results.append(result)
                
                # Log result
                with open(self.results_log, 'a') as f:
                    f.write(json.dumps(result) + '\n')
                
                # Print verification summary
                if result.get("status") == "success" and "verification_results" in result:
                    verif = result["verification_results"]
                    print(f"   âœ… Data completeness: {verif['data_completeness']}")
                    print(f"   âœ… Warning displayed: {verif['warning_displayed']}")
                    print(f"   âœ… Confidence reduced: {verif['confidence_reduced']}")
                    print(f"   âœ… UX compliance: {verif['ux_compliance']['overall_compliance']}")
                
            except Exception as e:
                print(f"âŒ Critical error: {e}")
                error_result = {
                    "profile": profile,
                    "status": "critical_error",
                    "error": str(e)
                }
                results.append(error_result)
        
        # Generate compliance report
        print("\nðŸ“‹ Generating compliance report...")
        compliance_report = self.generate_compliance_report(results)
        
        print("\n" + "=" * 60)
        print(compliance_report)
        print("=" * 60)
        
        # Summary statistics
        total_tests = len(results)
        passed_tests = len([r for r in results if r.get("status") == "success"])
        
        print(f"\nðŸ“ˆ Test Summary:")
        print(f"   Total profiles: {total_tests}")
        print(f"   Successful tests: {passed_tests}")
        print(f"   Success rate: {passed_tests/total_tests*100:.1f}%")
        
        # Check screenshot generation
        screenshot_count = len([r for r in results if "screenshot_metadata" in r])
        print(f"\nðŸ“¸ Screenshots generated: {screenshot_count}")
        
        return {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "success_rate": passed_tests/total_tests*100,
            "results": results,
            "compliance_report": compliance_report
        }

def main():
    """Main execution function."""
    tester = DataCompletenessTester()
    
    try:
        results = tester.run_data_completeness_tests()
        
        # Save final summary
        summary_file = Path("data_completeness_test_summary.json")
        with open(summary_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"\nðŸ’¾ Complete results saved to: {summary_file}")
        
        return results
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Test interrupted by user")
        return {"status": "interrupted"}
    except Exception as e:
        print(f"\n\nâŒ Critical test failure: {e}")
        return {"status": "critical_failure", "error": str(e)}

if __name__ == "__main__":
    results = main()
    sys.exit(0 if results.get("success_rate", 0) >= 80 else 1)